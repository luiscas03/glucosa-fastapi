from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field
from pathlib import Path
import joblib
import pandas as pd
import numpy as np
from typing import Dict, List, Optional
import logging
from datetime import datetime

# ===== CONFIGURACI√ìN DE LOGGING =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===== CONFIGURACI√ìN DE FASTAPI =====
app = FastAPI(
    title="Glucose ML Monitor API",
    description="API para predicci√≥n de glucosa usando 7 modelos ML ensemble",
    version="2.1.0"
)

# ===== CONFIGURACI√ìN DE CORS =====
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== RUTAS DE MODELOS =====
BASE_DIR = Path(__file__).resolve().parent
ROOT_DIR = BASE_DIR.parent
MODELS_DIR = ROOT_DIR / "assets" / "models" / "monitor"

MODEL_FILES = {
    "xgboost": MODELS_DIR / "XGBoost.joblib",
    "random_forest": MODELS_DIR / "Random_Forest.joblib",
    "lightgbm": MODELS_DIR / "LightGBM.joblib",
    "gradient_boosting": MODELS_DIR / "Gradient_Boosting.joblib",
    "ridge": MODELS_DIR / "Ridge.joblib",
    "lasso": MODELS_DIR / "Lasso.joblib",
    "elasticnet": MODELS_DIR / "ElasticNet.joblib"
}

PREPROCESSING_FILE = MODELS_DIR / "preprocessing_objects.pkl"

# ===== VARIABLES GLOBALES =====
models = {}
label_encoders = {}
scaler = None
FEATURE_NAMES = None

# ===== MAPEO DE NOMBRES: frontend (lowercase) ‚Üí sklearn (PascalCase) =====
FEATURE_MAP = {
    'Edad': 'Edad',
    'Sexo': 'Sexo',
    'Peso': 'Peso',
    'Talla': 'Talla',
    'IMC': 'IMC',
    'Perimetro_Cintura': 'Perimetro_Cintura',
    'SpO2': 'SpO2',
    'Frecuencia_Cardiaca': 'Frecuencia_Cardiaca',
    'Actividad_Fisica': 'Actividad_Fisica',
    'Consumo_Frutas': 'Consumo_Frutas',
    'Tiene_Hipertension': 'Tiene_Hipertension',
    'Tiene_Diabetes': 'Tiene_Diabetes',
    'Puntaje_FINDRISC': 'Puntaje_FINDRISC'
}

# ===== MODELO DE DATOS DE ENTRADA (lowercase para frontend) =====
class PredictionInput(BaseModel):
    edad: int = Field(..., ge=18, le=120, description="Edad del paciente")
    sexo: str = Field(..., description="Sexo: Masculino o Femenino")
    peso: float = Field(..., gt=0, description="Peso en kilogramos")
    talla: float = Field(..., gt=0, description="Talla en metros")
    imc: Optional[float] = Field(None, description="IMC (calculado si no se proporciona)")
    perimetro_cintura: float = Field(..., gt=0, description="Per√≠metro de cintura en cm")
    spo2: int = Field(..., ge=70, le=100, description="Saturaci√≥n de ox√≠geno")
    frecuencia_cardiaca: int = Field(..., ge=40, le=200, description="Frecuencia card√≠aca")
    actividad_fisica: str = Field(..., description="si o no")
    consumo_frutas: str = Field(..., description="si o no")
    tiene_hipertension: str = Field(..., description="Si o No")
    tiene_diabetes: str = Field(..., description="Si o No")
    puntaje_findrisc: int = Field(..., ge=0, le=26, description="Puntaje FINDRISC")

# ===== CARGAR MODELOS Y PREPROCESSING =====
@app.on_event("startup")
async def load_models():
    global models, label_encoders, scaler, FEATURE_NAMES
    
    try:
        logger.info("üöÄ Iniciando carga de modelos ML...")
        
        # 1. Cargar objetos de preprocesamiento
        if not PREPROCESSING_FILE.exists():
            raise FileNotFoundError(f"Archivo de preprocesamiento no encontrado: {PREPROCESSING_FILE}")
        
        preprocessing = joblib.load(PREPROCESSING_FILE)
        label_encoders = preprocessing.get('label_encoders', {})
        scaler = preprocessing.get('scaler', None)
        FEATURE_NAMES = preprocessing.get('feature_names', None)
        
        logger.info(f"‚úÖ Preprocesamiento cargado")
        logger.info(f"üìã Feature Names: {FEATURE_NAMES}")
        
        if FEATURE_NAMES is None:
            logger.warning("‚ö†Ô∏è feature_names no encontrado en preprocessing_objects.pkl")
        
        # 2. Cargar cada modelo ML
        models_loaded = 0
        for model_name, model_path in MODEL_FILES.items():
            if not model_path.exists():
                logger.warning(f"‚ö†Ô∏è Modelo no encontrado: {model_path}")
                continue
            
            try:
                models[model_name] = joblib.load(model_path)
                models_loaded += 1
                logger.info(f"‚úÖ Modelo cargado: {model_name}")
            except Exception as e:
                logger.error(f"‚ùå Error al cargar {model_name}: {str(e)}")
        
        if models_loaded == 0:
            raise RuntimeError("No se pudo cargar ning√∫n modelo ML")
        
        logger.info(f"üéâ Total de modelos cargados: {models_loaded}/7")
        
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico al cargar modelos: {str(e)}")
        raise

# ===== FUNCI√ìN DE PREPROCESAMIENTO =====
def preprocess_input(data: PredictionInput) -> pd.DataFrame:
    """
    Preprocesa los datos de entrada y RETORNA UN DATAFRAME con feature names.
    Mapea de lowercase (frontend) a PascalCase (sklearn models).
    """
    try:
        # 1. Calcular IMC si no viene
        imc_value = data.imc if data.imc else (data.peso / (data.talla ** 2))
        
        # 2. Construir diccionario con NOMBRES PASCALCASE (como espera sklearn)
        input_dict = {
            'Edad': data.edad,
            'Sexo': data.sexo.lower().capitalize(),  # "masculino" ‚Üí "Masculino"
            'Peso': data.peso,
            'Talla': data.talla,
            'IMC': imc_value,
            'Perimetro_Cintura': data.perimetro_cintura,
            'SpO2': data.spo2,
            'Frecuencia_Cardiaca': data.frecuencia_cardiaca,
            'Actividad_Fisica': data.actividad_fisica.lower().capitalize(),  # "si" ‚Üí "Si"
            'Consumo_Frutas': data.consumo_frutas.lower().capitalize(),
            'Tiene_Hipertension': data.tiene_hipertension.lower().capitalize(),
            'Tiene_Diabetes': data.tiene_diabetes.lower().capitalize(),
            'Puntaje_FINDRISC': data.puntaje_findrisc
        }
        
        logger.info(f"üìä Input dict (PascalCase): {input_dict}")
        
        # 3. Aplicar LabelEncoders a variables categ√≥ricas
        for col, encoder in label_encoders.items():
            if col in input_dict:
                try:
                    input_dict[col] = encoder.transform([input_dict[col]])[0]
                    logger.info(f"‚úÖ Encoded {col}: {input_dict[col]}")
                except ValueError as e:
                    logger.warning(f"‚ö†Ô∏è Valor no visto en {col}: {input_dict[col]}, usando valor por defecto 0")
                    input_dict[col] = 0
        
        # 4. Crear DataFrame con el orden correcto de columnas
        if FEATURE_NAMES is not None:
            # Usar el orden exacto del entrenamiento
            df = pd.DataFrame([{col: input_dict.get(col, 0) for col in FEATURE_NAMES}])
        else:
            df = pd.DataFrame([input_dict])
        
        logger.info(f"üìã DataFrame columns: {list(df.columns)}")
        
        # 5. Aplicar StandardScaler
        if scaler is not None:
            X_scaled = scaler.transform(df)
            X_scaled_df = pd.DataFrame(X_scaled, columns=df.columns)
            logger.info(f"‚úÖ Datos escalados correctamente")
            return X_scaled_df
        else:
            return df
        
    except Exception as e:
        logger.error(f"‚ùå Error en preprocesamiento: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error en preprocesamiento: {str(e)}")

# ===== ENDPOINT: HEALTH CHECK =====
@app.get("/health")
async def health_check():
    """Verifica el estado de la API y modelos cargados"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "models_loaded": len(models),
        "models_available": list(models.keys()),
        "feature_names_loaded": FEATURE_NAMES is not None
    }

# ===== ENDPOINT: PREDICCI√ìN =====
@app.post("/predict")
async def predict_glucose(data: PredictionInput):
    """
    Realiza predicci√≥n de glucosa usando ensemble de 7 modelos ML
    """
    try:
        logger.info(f"üì• Solicitud de predicci√≥n recibida: edad={data.edad}, sexo={data.sexo}")
        
        # 1. Preprocesar datos
        X_preprocessed = preprocess_input(data)
        logger.info(f"‚úÖ Datos preprocesados: shape={X_preprocessed.shape}")
        
        # 2. Realizar predicciones con cada modelo
        predicciones_individuales = {}
        predicciones_validas = []
        
        for model_name, model in models.items():
            try:
                pred = model.predict(X_preprocessed)[0]
                predicciones_individuales[model_name] = float(pred)
                predicciones_validas.append(pred)
                logger.info(f"‚úÖ {model_name}: {pred:.2f} mg/dL")
            except Exception as e:
                logger.error(f"‚ùå Error en modelo {model_name}: {str(e)}")
                predicciones_individuales[model_name] = None
        
        if len(predicciones_validas) == 0:
            raise HTTPException(status_code=500, detail="Ning√∫n modelo pudo generar predicci√≥n")
        
        # 3. Calcular ensemble (promedio)
        prediccion_final = float(np.mean(predicciones_validas))
        
        # 4. Clasificar categor√≠a de glucosa
        if prediccion_final < 100:
            categoria = "Normal"
        elif prediccion_final < 126:
            categoria = "Prediabetes"
        else:
            categoria = "Diabetes"
        
        # 5. Calcular m√©tricas de confianza
        std_predicciones = float(np.std(predicciones_validas))
        mae = std_predicciones
        confidence = 1.0 - (std_predicciones / prediccion_final) if prediccion_final > 0 else 0.5
        confidence = max(0.0, min(1.0, confidence))
        
        intervalo_min = prediccion_final - 1.96 * std_predicciones
        intervalo_max = prediccion_final + 1.96 * std_predicciones
        
        # 6. Identificar mejor modelo (m√°s cercano al promedio)
        mejor_modelo = min(
            [(k, v) for k, v in predicciones_individuales.items() if v is not None],
            key=lambda x: abs(x[1] - prediccion_final)
        )[0]
        
        # 7. Generar respuesta
        response = {
            "prediccion_final": round(prediccion_final, 2),
            "categoria": categoria,
            "predicciones_individuales": {
                k: round(v, 2) if v is not None else None 
                for k, v in predicciones_individuales.items()
            },
            "confidence": round(confidence, 3),
            "mae": round(mae, 2),
            "intervalo_confianza": [round(intervalo_min, 2), round(intervalo_max, 2)],
            "mejor_modelo": mejor_modelo,
            "timestamp": datetime.now().isoformat(),
            "input_data": data.dict()
        }
        
        logger.info(f"‚úÖ Predicci√≥n completada: {prediccion_final:.2f} mg/dL ({categoria})")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error en predicci√≥n: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error en predicci√≥n: {str(e)}")

# ===== ENDPOINT: SERVIR INDEX.HTML =====
@app.get("/")
async def serve_index():
    """Sirve el archivo index.html de la interfaz web"""
    index_path = BASE_DIR / "index.html"
    
    if not index_path.exists():
        raise HTTPException(status_code=404, detail="index.html no encontrado")
    
    return FileResponse(index_path)

# ===== ENDPOINT: LISTAR MODELOS =====
@app.get("/models")
async def list_models():
    """Lista todos los modelos ML cargados"""
    return {
        "models": list(models.keys()),
        "total": len(models),
        "feature_names": FEATURE_NAMES
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
